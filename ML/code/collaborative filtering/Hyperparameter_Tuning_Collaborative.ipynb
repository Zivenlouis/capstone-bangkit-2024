{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkBcPL7FmCPz",
        "outputId": "e6542ce7-4b78-466b-8702-a75dbba1c9dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.6.2)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import model_from_json\n",
        "import keras_tuner as kt\n",
        "\n",
        "# Load data\n",
        "df_phone_dataset = pd.read_csv('phone_dataset.csv')\n",
        "df_user_clicks = pd.read_csv('user_clicks_1_brand.csv')\n",
        "df_user_ratings = pd.read_csv('user_ratings_1_brand.csv')\n",
        "\n",
        "PHONE_COUNT = len(df_phone_dataset)\n",
        "MIN_USER_ID = min(df_user_clicks['user_id'].min(), df_user_ratings['user_id'].min())\n",
        "MAX_USER_ID = max(df_user_clicks['user_id'].max(), df_user_ratings['user_id'].max())\n",
        "TOTAL_USER = MAX_USER_ID - MIN_USER_ID + 1\n",
        "TAKE_RATING_PER_USER = 20\n",
        "TAKE_CLICK_PER_USER = 20\n",
        "\n",
        "# Preprocess user_clicks\n",
        "df_user_clicks = df_user_clicks.sort_values(by='visit_time', ascending=False).drop_duplicates(subset=['user_id', 'phone_id'], keep='first').groupby('user_id').head(TAKE_CLICK_PER_USER).reset_index(drop=True)\n",
        "missing_user_ids = set(range(MIN_USER_ID, MAX_USER_ID + 1)) - set(df_user_clicks['user_id'])\n",
        "missing_data = pd.DataFrame({'user_id': list(missing_user_ids), 'phone_id': -1, 'visit_time': '2024-01-01 00:00:00'})\n",
        "df_user_clicks_complete = pd.concat([df_user_clicks, missing_data], ignore_index=True).sort_values(by='user_id').reset_index(drop=True)\n",
        "train_clicks = df_user_clicks_complete[['user_id', 'phone_id']].pivot_table(index='phone_id', columns='user_id', aggfunc=lambda x: 1 if len(x) > 0 else 0, fill_value=0)\n",
        "if len(missing_user_ids) > 0:\n",
        "    train_clicks = train_clicks[1:]\n",
        "\n",
        "# Preprocess user_ratings\n",
        "df_user_ratings = df_user_ratings.sort_values(by='rate_time', ascending=False).drop_duplicates(subset=['user_id', 'phone_id'], keep='first').groupby('user_id').head(TAKE_RATING_PER_USER).reset_index(drop=True)\n",
        "missing_user_ids = set(range(MIN_USER_ID, MAX_USER_ID + 1)) - set(df_user_ratings['user_id'])\n",
        "missing_data = pd.DataFrame({'user_id': list(missing_user_ids), 'phone_id': -1, 'rating': 0})\n",
        "df_user_ratings_complete = pd.concat([df_user_ratings, missing_data], ignore_index=True).sort_values(by='user_id').reset_index(drop=True)\n",
        "train_ratings = df_user_ratings_complete.pivot(index='phone_id', columns='user_id', values='rating').fillna(0)\n",
        "if len(missing_user_ids) > 0:\n",
        "    train_ratings = train_ratings[1:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hp):\n",
        "    click_input = tf.keras.Input(shape=(PHONE_COUNT,), dtype=tf.int32, name='click_input')\n",
        "    rating_input = tf.keras.Input(shape=(PHONE_COUNT,), dtype=tf.int32, name='rating_input')\n",
        "\n",
        "    # Tune the number of units in the dense layer\n",
        "    num_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "    X = tf.keras.layers.Concatenate(name='concatenated_inputs')([rating_input, click_input])\n",
        "    X = tf.keras.layers.Dense(units=num_units, activation='relu', kernel_initializer='random_normal', name='X')(X)\n",
        "\n",
        "    # Tune the activation function\n",
        "    activation = hp.Choice('activation', values=['relu', 'tanh', 'sigmoid'])\n",
        "    X = tf.keras.layers.Activation(activation)(X)\n",
        "\n",
        "    # Add a second layer if chosen\n",
        "    if hp.Boolean('use_second_layer'):\n",
        "        num_units_second = hp.Int('units_second', min_value=32, max_value=512, step=32)\n",
        "        X = tf.keras.layers.Dense(units=num_units_second, activation=activation, kernel_initializer='random_normal', name='X2')(X)\n",
        "\n",
        "    output_click = tf.keras.layers.Dense(PHONE_COUNT, activation='sigmoid', name='output')(X)\n",
        "    model = tf.keras.Model(inputs=[rating_input, click_input], outputs=output_click, name='collaborative_model')\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(\n",
        "                      hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-3)),\n",
        "                  loss=tf.keras.losses.MeanSquaredError())\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "tb_EmqrvmT2V"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_epochs=30,\n",
        "    hyperband_iterations=2,\n",
        "    directory='my_dir',\n",
        "    project_name='hyperparam_tuning')\n"
      ],
      "metadata": {
        "id": "1H9gTAWBmWsD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "click_input = train_clicks.to_numpy().T\n",
        "\n",
        "# Normalize ratings\n",
        "train_ratings = train_ratings / 5\n",
        "rating_input = train_ratings.to_numpy().T\n",
        "\n",
        "tuner.search([rating_input, click_input], [rating_input, click_input], epochs=30, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(best_hps.values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6J_sSfGmYYW",
        "outputId": "4629e0d2-34f2-42b9-8078-638d03f2f1b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 180 Complete [00h 00m 07s]\n",
            "val_loss: 0.037413787096738815\n",
            "\n",
            "Best val_loss So Far: 0.03696547448635101\n",
            "Total elapsed time: 00h 07m 54s\n",
            "{'units': 128, 'activation': 'sigmoid', 'use_second_layer': True, 'learning_rate': 0.0014618661689036517, 'units_second': 352, 'tuner/epochs': 2, 'tuner/initial_epoch': 0, 'tuner/bracket': 3, 'tuner/round': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit([rating_input, click_input], [rating_input, click_input], epochs=30, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx7Vqtlmol88",
        "outputId": "6025aa3e-619a-42a3-c3b4-7ce7e7cb48e7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "16/16 [==============================] - 1s 6ms/step - loss: 0.0841\n",
            "Epoch 2/30\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0395\n",
            "Epoch 3/30\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0396\n",
            "Epoch 4/30\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0394\n",
            "Epoch 5/30\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0392\n",
            "Epoch 6/30\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0390\n",
            "Epoch 7/30\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0388\n",
            "Epoch 8/30\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0387\n",
            "Epoch 9/30\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0387\n",
            "Epoch 10/30\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0387\n",
            "Epoch 11/30\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0386\n",
            "Epoch 12/30\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0386\n",
            "Epoch 13/30\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0386\n",
            "Epoch 14/30\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0385\n",
            "Epoch 15/30\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0384\n",
            "Epoch 16/30\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0383\n",
            "Epoch 17/30\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0381\n",
            "Epoch 18/30\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0378\n",
            "Epoch 19/30\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0372\n",
            "Epoch 20/30\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0361\n",
            "Epoch 21/30\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0347\n",
            "Epoch 22/30\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0336\n",
            "Epoch 23/30\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.0330\n",
            "Epoch 24/30\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0326\n",
            "Epoch 25/30\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0323\n",
            "Epoch 26/30\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0320\n",
            "Epoch 27/30\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0317\n",
            "Epoch 28/30\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.0313\n",
            "Epoch 29/30\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0307\n",
            "Epoch 30/30\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0299\n"
          ]
        }
      ]
    }
  ]
}